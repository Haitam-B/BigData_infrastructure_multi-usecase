FROM alpine:latest

# Update and install necessary packages
RUN apk add --update --no-cache python3 py3-pip curl bash openjdk8-jre gcc g++ python3-dev musl-dev linux-headers bash

ENV SPARK_VERSION=3.5.1
ENV HADOOP_VERSION=3.2.1

# Install Hadoop
RUN curl -L https://archive.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz | tar -xz -C /opt/ && \
    mv /opt/hadoop-${HADOOP_VERSION} /opt/hadoop

# Install Spark
RUN curl -L https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop3.tgz | tar -xz -C /opt/ && \
    mv /opt/spark-${SPARK_VERSION}-bin-hadoop3 /opt/spark

# Set environment variables for Hadoop and Spark
ENV HADOOP_HOME=/opt/hadoop
ENV SPARK_HOME=/opt/spark
ENV PATH=$HADOOP_HOME/bin:$SPARK_HOME/bin:$PATH

# Install Jupyter Notebook and PySpark
RUN pip3 install notebook pyspark --break-system-packages

# Copy Hadoop and Spark configuration files (make sure these directories exist in your build context)
COPY hadoop-config /opt/hadoop/etc/hadoop
COPY spark-config /opt/spark/conf

# Expose the Jupyter Notebook port
EXPOSE 8888

# Set the working directory
WORKDIR /app

# Command to start Jupyter Notebook
CMD ["jupyter", "notebook", "--ip=0.0.0.0", "--port=8888", "--no-browser", "--allow-root"]
